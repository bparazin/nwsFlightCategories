{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47de7742-58fa-49b6-87ae-fafbaf269bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from metar import Metar\n",
    "import numpy as np\n",
    "from IOfuncs import *\n",
    "import datetime as dt\n",
    "import warnings\n",
    "from json import JSONDecodeError\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFdr,SelectFpr,f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "900af016-02aa-4d3a-bfd0-c84e534b68a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ml_data_row(taf_time, station, lat, lon, metar_path, glamp_path, hrrr_path, delay_hours = 2):\n",
    "    if isinstance(metar_path, str):\n",
    "        metar_path = read_metar(metar_path)\n",
    "    metarDF = pd.DataFrame()\n",
    "    glampDF = pd.DataFrame()\n",
    "    hrrrDF = pd.DataFrame()    \n",
    "    \n",
    "    for time in range(-6, -delay_hours, 1):\n",
    "        metar_at_time = get_metar_at_time(taf_time + dt.timedelta(hours = time), metar_path).T\n",
    "        metarDF[f'metar {time}'] = metar_at_time\n",
    "    \n",
    "    work_time = dt.timedelta(hours=-delay_hours)\n",
    "    glamp_data = get_glamp_at_time(taf_time + work_time, glamp_path, station, download=True)\n",
    "    hrrr_data = get_hrrr_at_time(taf_time + work_time, hrrr_path, lat, lon, download=True)\n",
    "    glamp_synoptic_offset = (taf_time.hour - delay_hours) % 6 - 1\n",
    "    for time in range(-delay_hours, 7, 1):\n",
    "        glampDF[f'glamp {time}'] = glamp_data.iloc[time + delay_hours + glamp_synoptic_offset]\n",
    "        hrrrDF[f'hrrr {time}'] = hrrr_data.iloc[time + delay_hours]    \n",
    "        \n",
    "    \n",
    "    df = pd.concat([metarDF, glampDF, hrrrDF])\n",
    "    df.drop(['ftime', 'ftime_utc', 'model', 'runtime', 'runtime_utc', 'station', 'metar', 'peak_wind_time', 'valid', 'Unnamed: 0'], inplace=True)\n",
    "\n",
    "    v = df.unstack().to_frame().sort_index(level=1).T\n",
    "    v.columns = v.columns.map('_'.join)\n",
    "\n",
    "    final = v.dropna(axis = 1)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bb22f87-70cf-43d3-bd2a-89ac83e535c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "taf_time = dt.datetime(year = 2021, month = 8, day = 21, hour = 18, minute = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2de18cd0-3b6e-40ac-8543-622a0c1c0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ml_training_data_row(taf_time, station, lat, lon, metar_path, glamp_path, hrrr_path, asos5_path, delay_hours = 2, tplus_hours = 6):\n",
    "    if isinstance(metar_path, str):\n",
    "        metar_path = full_metar_list = read_metar(metar_path)\n",
    "    if isinstance(asos5_path, str):\n",
    "        asos5_path = read_metar(asos5_path)\n",
    "    \n",
    "    df = make_ml_data_row(taf_time, station, lat, lon, metar_path, glamp_path, hrrr_path, delay_hours = delay_hours)\n",
    "    \n",
    "    for i in range(tplus_hours):\n",
    "        df[f'flight category {i}'] = get_conditions_from_asos(taf_time + dt.timedelta(hours = i), metar_path)\n",
    "        time_series = pd.date_range(taf_time + dt.timedelta(hours = i), taf_time + dt.timedelta(hours = i+1), freq = '5T')\n",
    "        verification_series = np.asarray(asos5_path.truncate(before = taf_time + dt.timedelta(hours = i), \n",
    "                                                             after = taf_time + dt.timedelta(hours = i+1))['conditions'])\n",
    "        df[f'verification list {i}'] = [None]\n",
    "        df[f'verification list {i}'][0] = verification_series\n",
    "        \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5671b297-de44-48ab-8021-c82e658fff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metar = read_metar('Data/BOS.csv')\n",
    "asos5 = read_metar('Data/BOS_5min.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70d50f25-2915-47d0-8b95-9939441fc0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 94.3 ms, sys: 0 ns, total: 94.3 ms\n",
      "Wall time: 134 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hrrr -1_DPT_1000mb</th>\n",
       "      <th>hrrr -2_DPT_1000mb</th>\n",
       "      <th>hrrr 0_DPT_1000mb</th>\n",
       "      <th>hrrr 1_DPT_1000mb</th>\n",
       "      <th>hrrr 2_DPT_1000mb</th>\n",
       "      <th>hrrr 3_DPT_1000mb</th>\n",
       "      <th>hrrr 4_DPT_1000mb</th>\n",
       "      <th>hrrr 5_DPT_1000mb</th>\n",
       "      <th>hrrr 6_DPT_1000mb</th>\n",
       "      <th>hrrr -1_DPT_2m_above_ground</th>\n",
       "      <th>...</th>\n",
       "      <th>flight category 1</th>\n",
       "      <th>verification list 1</th>\n",
       "      <th>flight category 2</th>\n",
       "      <th>verification list 2</th>\n",
       "      <th>flight category 3</th>\n",
       "      <th>verification list 3</th>\n",
       "      <th>flight category 4</th>\n",
       "      <th>verification list 4</th>\n",
       "      <th>flight category 5</th>\n",
       "      <th>verification list 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>292.8</td>\n",
       "      <td>293.5</td>\n",
       "      <td>292.2</td>\n",
       "      <td>292.0</td>\n",
       "      <td>291.8</td>\n",
       "      <td>294.0</td>\n",
       "      <td>293.5</td>\n",
       "      <td>292.2</td>\n",
       "      <td>293.2</td>\n",
       "      <td>293.2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 523 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  hrrr -1_DPT_1000mb hrrr -2_DPT_1000mb hrrr 0_DPT_1000mb hrrr 1_DPT_1000mb  \\\n",
       "0              292.8              293.5             292.2             292.0   \n",
       "\n",
       "  hrrr 2_DPT_1000mb hrrr 3_DPT_1000mb hrrr 4_DPT_1000mb hrrr 5_DPT_1000mb  \\\n",
       "0             291.8             294.0             293.5             292.2   \n",
       "\n",
       "  hrrr 6_DPT_1000mb hrrr -1_DPT_2m_above_ground  ... flight category 1  \\\n",
       "0             293.2                       293.2  ...                 3   \n",
       "\n",
       "                          verification list 1 flight category 2  \\\n",
       "0  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]                 3   \n",
       "\n",
       "                    verification list 2 flight category 3  \\\n",
       "0  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]                 3   \n",
       "\n",
       "                          verification list 3 flight category 4  \\\n",
       "0  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]                 3   \n",
       "\n",
       "                    verification list 4 flight category 5  \\\n",
       "0  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]                 3   \n",
       "\n",
       "                    verification list 5  \n",
       "0  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]  \n",
       "\n",
       "[1 rows x 523 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "make_ml_training_data_row(taf_time, 'kbos', 42.3656, -71.0096, metar, 'Data/GLAMP data/', 'Data/hrrr/', asos5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc301d69-8ab5-40e9-b08c-6db2c8e74602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ml_training_data_set(start_time, end_time, station, lat, lon, metar_path, glamp_path, hrrr_path, asos5_path, delay_hours = 2, frequency = '5H'):\n",
    "    training_df = pd.DataFrame()\n",
    "    time_series = pd.date_range(start_time, end_time, freq = frequency)\n",
    "    if isinstance(metar_path, str):\n",
    "        metar_path = read_metar(metar_path)\n",
    "    if isinstance(asos5_path, str):\n",
    "        asos5_path = read_metar(asos5_path)\n",
    "    for time in tqdm(time_series):\n",
    "        try:\n",
    "            training_row = make_ml_training_data_row(time, station, lat, lon, metar_path, glamp_path, hrrr_path, asos5_path, delay_hours = delay_hours)\n",
    "            training_df = pd.concat([training_df, training_row])\n",
    "        except (FileNotFoundError, JSONDecodeError):\n",
    "            continue\n",
    "\n",
    "    training_df = training_df.fillna(-99999)\n",
    "    return training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07d4d136-d999-44e2-bb76-21aaf6bb0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_of_detection(predicted_results, actual_results, flight_cat):\n",
    "    result_locations = np.where(actual_results==flight_cat)[0]\n",
    "    num_predict = np.sum(predicted_results[result_locations]==flight_cat)\n",
    "    return num_predict / len(result_locations)\n",
    "\n",
    "def false_alarm_rate(predicted_results, actual_results, flight_cat):\n",
    "    num_predict = np.sum(predicted_results==flight_cat)\n",
    "    predict_locations = np.where(predicted_results==flight_cat)[0]\n",
    "    predict_subset = predicted_results[predict_locations]\n",
    "    actual_subset = actual_results.iloc[predict_locations]\n",
    "    \n",
    "    false_alarm_count = np.sum(predict_subset!=actual_subset)\n",
    "    \n",
    "    return false_alarm_count/num_predict\n",
    "\n",
    "def critical_success_index(predicted_results, actual_results, flight_cat):\n",
    "    num_predict = np.sum(predicted_results==flight_cat)\n",
    "    predict_locations = np.where(predicted_results==flight_cat)[0]\n",
    "    non_predict_locations = np.where(predicted_results!=flight_cat)[0]\n",
    "    predict_subset = predicted_results[predict_locations]\n",
    "    actual_subset = actual_results.iloc[predict_locations]\n",
    "    actual_subset_compliment = actual_results.iloc[non_predict_locations]\n",
    "    \n",
    "    hits = np.sum(predict_subset==actual_subset)\n",
    "    false_alarm_count = np.sum(predict_subset!=actual_subset)\n",
    "    misses = np.sum(actual_subset_compliment==flight_cat)\n",
    "    \n",
    "    return hits / (hits + false_alarm_count + misses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a32a3828-bc9c-4690-a3a7-0e673cb857cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_of_detection_nfv(predicted_results, actual_results):\n",
    "    result_locations = np.where(actual_results!=3)[0]\n",
    "    num_predict = np.sum(predicted_results[result_locations]!=3)\n",
    "    return num_predict / len(result_locations)\n",
    "\n",
    "def false_alarm_rate_nfv(predicted_results, actual_results):\n",
    "    num_predict = np.sum(predicted_results!=3)\n",
    "    predict_locations = np.where(predicted_results!=3)[0]\n",
    "    predict_subset = predicted_results[predict_locations]\n",
    "    actual_subset = actual_results.iloc[predict_locations]\n",
    "    \n",
    "    false_alarm_count = np.sum(predict_subset!=actual_subset)\n",
    "    \n",
    "    return false_alarm_count/num_predict\n",
    "\n",
    "def critical_success_index_nfv(predicted_results, actual_results):\n",
    "    num_predict = np.sum(predicted_results!=3)\n",
    "    predict_locations = np.where(predicted_results!=3)[0]\n",
    "    non_predict_locations = np.where(predicted_results==3)[0]\n",
    "    predict_subset = predicted_results[predict_locations]\n",
    "    actual_subset = actual_results.iloc[predict_locations]\n",
    "    actual_subset_compliment = actual_results.iloc[non_predict_locations]\n",
    "    \n",
    "    hits = np.sum(predict_subset==actual_subset)\n",
    "    false_alarm_count = np.sum(predict_subset!=actual_subset)\n",
    "    misses = np.sum(actual_subset_compliment!=3)\n",
    "    \n",
    "    return hits / (hits + false_alarm_count + misses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebb51f0b-0872-4fc0-a6d9-b0b8f409c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data):\n",
    "    y_keys = np.asarray([key for key in data if 'flight category' in key])\n",
    "    val_keys = np.asarray([key for key in data if 'verification list' in key])\n",
    "    X = data.drop(np.concatenate([y_keys, val_keys]), axis=1)\n",
    "    y_list = data[y_keys]\n",
    "    val_list = data[val_keys]\n",
    "    \n",
    "    return X, y_list, val_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ee189a5-973e-4c8a-83c1-5ecfe91fbee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24105/24105 [16:10:36<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "start_date = dt.datetime(year = 2020, month = 1, day = 1, hour = 0, minute = 0)\n",
    "end_date = dt.datetime.now()\n",
    "data = make_ml_training_data_set(start_date, end_date, 'kbos', 42.3656, -71.0096, 'Data/BOS.csv', 'Data/GLAMP data/', \n",
    "                                 'Data/hrrr/', 'Data/BOS_5min.csv', frequency = 'H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55bc23-ebb7-4a06-b0fe-c18630102611",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_winter = dt.datetime(year = 2020, month = 12, day = 1, hour = 0, minute = 0)\n",
    "end_date_winter = dt.datetime(year = 2021, month = 2, day = 28, hour = 23, minute = 0)\n",
    "winter_data = make_ml_training_data_set(start_date_winter, end_date_winter, 'kbos', 42.3656, -71.0096, 'Data/BOS.csv',\n",
    "                                        'Data/GLAMP data/', 'Data/hrrr/', 'Data/BOS_5min.csv', frequency = 'H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b28e9-1d01-4aaa-b42a-8070085d307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_spring = dt.datetime(year = 2021, month = 3, day = 1, hour = 0, minute = 0)\n",
    "end_date_spring = dt.datetime(year = 2021, month = 5, day = 31, hour = 23, minute = 0)\n",
    "spring_data = make_ml_training_data_set(start_date_spring, end_date_spring, 'kbos', 42.3656, -71.0096, 'Data/BOS.csv', \n",
    "                                        'Data/GLAMP data/', 'Data/hrrr/', 'Data/BOS_5min.csv', frequency = 'H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ead147-65f0-4b02-8c73-84c4326b9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy_metrics(classifier_rf, training_data, flight_cat):\n",
    "    result_df = pd.DataFrame()\n",
    "    (X, y_list, val_list) = data_split(training_data)\n",
    "    for y in y_list:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_list[y], train_size=0.7, random_state=42)\n",
    "        classifier_rf.fit(X_train, y_train)\n",
    "        prob = classifier_rf.predict(X_test)\n",
    "        result_df[y] = (prob_of_detection(prob, y_test, flight_cat), \n",
    "                        false_alarm_rate(prob, y_test, flight_cat), \n",
    "                        critical_success_index(prob, y_test, flight_cat))\n",
    "        result_df.rename({0: 'POD', 1: 'FAR', 2: 'CSI'}, inplace=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f9126-6fef-4852-929a-7dfdc8805ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_rf = RandomForestClassifier(random_state=42, n_jobs = -1)\n",
    "classifier_nn = MLPClassifier(random_state=42)\n",
    "#params determined via hyperparam tuning\n",
    "n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
    "max_depth = [x for x in range(10, 120, 10)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "tuned_rf = RandomizedSearchCV(estimator = classifier_rf, param_distributions = random_grid, random_state = 42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f4092a-bc97-4f2a-9c26-f37bf59f11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy_metrics_nfv(classifier_rf, training_data):\n",
    "    result_df = pd.DataFrame()\n",
    "    (X, y_list) = data_split(training_data)\n",
    "    for y in y_list:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_list[y], train_size=0.7, random_state=42)\n",
    "        classifier_rf.fit(X_train, y_train)\n",
    "        prob = classifier_rf.predict(X_test)\n",
    "        #result_df[y] = (prob_of_detection_nfv(prob, y_test), \n",
    "                        #false_alarm_rate_nfv(prob, y_test), \n",
    "                        #critical_success_index_nfv(prob, y_test))\n",
    "        #result_df.rename({0: 'POD', 1: 'FAR', 2: 'CSI'}, inplace=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad0564-35f3-4a51-8efc-21989b9a6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_accuracy_metrics(classifier_rf, data, 3) #VFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a08bf8-10dc-405c-9323-bebcaad685b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_accuracy_metrics(classifier_rf, winter_data, 3) #VFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93c0ba-700e-4ed6-8803-f68d35df9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_accuracy_metrics(classifier_rf, data, 2) #MVFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fffb07-5777-4b7c-8759-062646275066",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_accuracy_metrics(classifier_rf, winter_data, 2) #MVFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e91b13c-59b4-44ea-b63f-c0258a6adc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_accuracy_metrics(classifier_rf, data, 1) #IFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a461b8-73ee-42f8-9dcb-996a8dfb526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_accuracy_metrics(classifier_rf, winter_data, 1) #IFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4fa32-b8f1-45bf-9b43-c0913c7b670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_accuracy_metrics(classifier_rf, data, 0) #LIFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe6008-57d3-4d35-853c-fddb96e7232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_accuracy_metrics(classifier_rf, winter_data, 0) #LIFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891ee79-17c7-4a99-935c-3a87bbaf196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_accuracy_metrics_nfv(classifier_rf, data) #Non-VFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93fb50-7952-4996-a77b-8d037467138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_accuracy_metrics_nfv(classifier_rf, winter_data) #Non-VFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0326811a-31dd-4e90-905a-83dc63f4c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "(X, y_list) = data_split(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_list['flight category 0'], train_size=0.7, random_state=42)\n",
    "classifier_rf.fit(X_train, y_train)\n",
    "\n",
    "imports = classifier_rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0c80a-872a-4944-9564-e9409d116dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importances = [(feature, sig) for feature, sig in zip(classifier_rf.feature_names_in_, imports)]\n",
    "importances.sort(key = lambda x: x[1])\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ffd62-e8d5-4a56-8872-bc1690633bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = permutation_importance(\n",
    "    classifier_rf, X_test, y_test, random_state=42, n_jobs=-1\n",
    ")\n",
    "#takes about 10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fc76e-b673-46e4-8101-158741b6e8b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importances = [(feature, sig) for feature, sig in zip(classifier_rf.feature_names_in_, result['importances_mean'])]\n",
    "#importances.sort(key = lambda x: x[0].split('_')[1])\n",
    "importances.sort(key = lambda x: -x[1])\n",
    "importances = pd.DataFrame(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06cabf-1513-4184-b2ca-a0af3f0ea372",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(importances[1]), np.min(importances[1]), np.mean(importances[1]), np.std(importances[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4dcd4-2d95-473b-9729-acbd645f8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(importances[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2d058-07c2-4201-b42d-1c6e875b2293",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 999):\n",
    "    print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b07e7d-5ffe-45c2-b6e3-9e55227a1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c4712-838e-4246-8ecf-1f6a00feb8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "condition_list = []\n",
    "asos5 = read_metar('Data/BOS_5min.csv')\n",
    "for _, metar_at_time in tqdm(asos5.iterrows()):\n",
    "    vis = metar_at_time['vsby']\n",
    "    cld_list = np.asarray(metar_at_time[['skyc1', 'skyc2', 'skyc3', 'skyc4']])\n",
    "    hgt_list = np.asarray(metar_at_time[['skyl1', 'skyl2', 'skyl3', 'skyl4']])\n",
    "    ovc_hgt = 100000\n",
    "    bkn_hgt = 100000\n",
    "\n",
    "    if 3 in list(cld_list):\n",
    "        ovc_hgt = hgt_list[cld_list == 3]\n",
    "        if len(ovc_hgt) > 1:\n",
    "            ovc_hgt = np.min(ovc_hgt)\n",
    "    if 2 in list(cld_list):\n",
    "        bkn_hgt = hgt_list[cld_list == 2]\n",
    "        if len(bkn_hgt) > 1:\n",
    "            bkn_hgt = np.min(bkn_hgt)\n",
    "    ceiling = np.min([ovc_hgt, bkn_hgt])\n",
    "\n",
    "    if ceiling < 500 or vis < 1:\n",
    "        conditions = 0\n",
    "    elif ceiling < 1000 or vis < 3:\n",
    "        conditions = 1\n",
    "    elif ceiling < 3000 or vis < 5:\n",
    "        conditions = 2\n",
    "    else:\n",
    "        conditions = 3\n",
    "    condition_list.append(conditions)\n",
    "asos5['conditions'] = condition_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2546db-c4ab-4f02-a3ab-0bc4eb7210f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "asos5.to_csv('Data/BOS_5min.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3c6e4-d7b7-44c3-8d8d-6762bb354e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
